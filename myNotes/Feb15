Feb25 actually
Web Scraping{
	HTML parse: python Beautifulsoup
	JSON parse: python json
	Store the data{
		MySQL, SQL, etc
	}
	retrieve webpages{
		cmd = "curl -L -m 20 '%s' > %s" % (url, out_file)
		os.system(cmd)

		or

		requests.get(url)
		html = response.text.encode('utf-8')

		>>soup = Beautifulsoup(html_file)
		>>soup.find_all('a')
		or
		>>soup.find_all('a' , id='link3')
		>>for link in soup.find_all('a'):
		>>	  print(link.get('href'))
	}
	Common prictice{
		when collecting data{
			'inspect element': helpful to get the format of the HTTP request + headers
			Do Not make HTTP requests too fast
			mimic the 'real' http requests as much as possible{
				set HTTP headers: User-agent, Host, Referer, etc.
			}
			Use Website API(if exists)(e.g. Airbnb API)
		}
	}
}